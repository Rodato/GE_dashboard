scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?m=256&fromDate=01%2F01%2F2020&countries=191&tillDate=31%2F12%2F2020&cbspeeches_page={page}&cbspeeches_page_length=25"))
scraping_path
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
chromever = NULL,
port = 4444L)
remote_driver <- driver[["client"]]
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages
metadata <- vector(mode = "list", length = nb_pages)
for(page in 1:nb_pages){
remote_driver$navigate(eval(scraping_path))
nod <- nod(session, eval(scraping_path)) # introducing to the new page
Sys.sleep(session$delay) # using the delay time set by polite
metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>%
map_chr(., ~.$getElementText()[[1]]),
info = remote_driver$findElements("css selector", ".item_date+ td") %>%
map_chr(., ~.$getElementText()[[1]]),
url = remote_driver$findElements("css selector", ".dark") %>%
map_chr(., ~.$getElementAttribute("href")[[1]]))
}
metadata <- bind_rows(metadata) %>%
separate(info, c("title", "description", "speaker"), "\n")
head(metadata)
write.csv2(metadata,
"C:/Users/danie/Documents/Investigación/MacroTextMining/data/central_bank_database/BIS/metadata/metadataBIS2020.csv")
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("[\\sitems]") %>%
as.integer()
if(nb_items == nrow(metadata)){
cat("You have extracted the metadata of all speeches")
} else{
cat("There is a problem. You have probably missed some speeches.")
}
#Download the pdf
pdf_to_download <- metadata %>%
mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
pdf_name = str_c(pdf_name, ".pdf")) %>%
pull(pdf_name)
polite_download <- function(domain, url, ...){
bow(domain) %>%
nod(glue(url)) %>%
rip(...)
}
tryCatch(
{
walk(pdf_to_download, ~polite_download(domain = bis_website_path,
url = glue("review/{.}"),
path = here(bis_data_path, "pdf"),))
},
error = function(e) {
print(glue("No pdf exist"))
}
)
##BIS Scraper
if(! "pacman" %in% installed.packages()){
install.packages("pacman", dependencies = TRUE)
}
pacman::p_load(tidyverse,
RSelenium,
polite,
glue,
here,
pdftools,
tesseract)
bis_data_path <- here(path.expand("~"), "Investigación","MacroTextMining","data","central_bank_database", "BIS")
bis_website_path <- "https://www.bis.org/"
session <- bow(bis_website_path)
session
cat(session$robotstxt$text)
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?m=256&fromDate=01%2F01%2F2018&countries=191&tillDate=31%2F12%2F2018&cbspeeches_page={page}&cbspeeches_page_length=25"))
scraping_path
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
chromever = NULL,
port = 4444L)
remote_driver <- driver[["client"]]
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages
metadata <- vector(mode = "list", length = nb_pages)
for(page in 1:nb_pages){
remote_driver$navigate(eval(scraping_path))
nod <- nod(session, eval(scraping_path)) # introducing to the new page
Sys.sleep(session$delay) # using the delay time set by polite
metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>%
map_chr(., ~.$getElementText()[[1]]),
info = remote_driver$findElements("css selector", ".item_date+ td") %>%
map_chr(., ~.$getElementText()[[1]]),
url = remote_driver$findElements("css selector", ".dark") %>%
map_chr(., ~.$getElementAttribute("href")[[1]]))
}
metadata <- bind_rows(metadata) %>%
separate(info, c("title", "description", "speaker"), "\n")
head(metadata)
write.csv2(metadata,
"C:/Users/danie/Documents/Investigación/MacroTextMining/data/central_bank_database/BIS/metadata/metadataBIS2018.csv")
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("[\\sitems]") %>%
as.integer()
if(nb_items == nrow(metadata)){
cat("You have extracted the metadata of all speeches")
} else{
cat("There is a problem. You have probably missed some speeches.")
}
#Download the pdf
pdf_to_download <- metadata %>%
mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
pdf_name = str_c(pdf_name, ".pdf")) %>%
pull(pdf_name)
polite_download <- function(domain, url, ...){
bow(domain) %>%
nod(glue(url)) %>%
rip(...)
}
tryCatch(
{
walk(pdf_to_download, ~polite_download(domain = bis_website_path,
url = glue("review/{.}"),
path = here(bis_data_path, "pdf"),))
},
error = function(e) {
print(glue("No pdf exist"))
}
)
##BIS Scraper
if(! "pacman" %in% installed.packages()){
install.packages("pacman", dependencies = TRUE)
}
pacman::p_load(tidyverse,
RSelenium,
polite,
glue,
here,
pdftools,
tesseract)
bis_data_path <- here(path.expand("~"), "Investigación","MacroTextMining","data","central_bank_database", "BIS")
bis_website_path <- "https://www.bis.org/"
session <- bow(bis_website_path)
session
cat(session$robotstxt$text)
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?m=256&fromDate=01%2F01%2F2017&countries=191&tillDate=31%2F12%2F2017&cbspeeches_page={page}&cbspeeches_page_length=25"))
scraping_path
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
chromever = NULL,
port = 4444L)
remote_driver <- driver[["client"]]
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("[\\sitems]") %>%
as.integer()
if(nb_items == nrow(metadata)){
cat("You have extracted the metadata of all speeches")
} else{
cat("There is a problem. You have probably missed some speeches.")
}
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages
metadata <- vector(mode = "list", length = nb_pages)
View(metadata)
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages
metadata <- vector(mode = "list", length = nb_pages)
##BIS Scraper
if(! "pacman" %in% installed.packages()){
install.packages("pacman", dependencies = TRUE)
}
pacman::p_load(tidyverse,
RSelenium,
polite,
glue,
here,
pdftools,
tesseract)
bis_data_path <- here(path.expand("~"), "Investigación","MacroTextMining","data","central_bank_database", "BIS")
bis_website_path <- "https://www.bis.org/"
session <- bow(bis_website_path)
session
cat(session$robotstxt$text)
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?m=256&fromDate=01%2F01%2F2016&countries=191&tillDate=31%2F12%2F2016&cbspeeches_page={page}&cbspeeches_page_length=25"))
scraping_path
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
chromever = NULL,
port = 4444L)
remote_driver <- driver[["client"]]
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages
metadata <- vector(mode = "list", length = nb_pages)
for(page in 1:nb_pages){
remote_driver$navigate(eval(scraping_path))
nod <- nod(session, eval(scraping_path)) # introducing to the new page
Sys.sleep(session$delay) # using the delay time set by polite
metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>%
map_chr(., ~.$getElementText()[[1]]),
info = remote_driver$findElements("css selector", ".item_date+ td") %>%
map_chr(., ~.$getElementText()[[1]]),
url = remote_driver$findElements("css selector", ".dark") %>%
map_chr(., ~.$getElementAttribute("href")[[1]]))
}
metadata <- bind_rows(metadata) %>%
separate(info, c("title", "description", "speaker"), "\n")
head(metadata)
write.csv2(metadata,
"C:/Users/danie/Documents/Investigación/MacroTextMining/data/central_bank_database/BIS/metadata/metadataBIS2016.csv")
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("[\\sitems]") %>%
as.integer()
if(nb_items == nrow(metadata)){
cat("You have extracted the metadata of all speeches")
} else{
cat("There is a problem. You have probably missed some speeches.")
}
#Download the pdf
pdf_to_download <- metadata %>%
mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
pdf_name = str_c(pdf_name, ".pdf")) %>%
pull(pdf_name)
polite_download <- function(domain, url, ...){
bow(domain) %>%
nod(glue(url)) %>%
rip(...)
}
tryCatch(
{
walk(pdf_to_download, ~polite_download(domain = bis_website_path,
url = glue("review/{.}"),
path = here(bis_data_path, "pdf"),))
},
error = function(e) {
print(glue("No pdf exist"))
}
)
View(metadata)
##BIS Scraper####
if(! "pacman" %in% installed.packages()){
install.packages("pacman", dependencies = TRUE)
}
pacman::p_load(tidyverse,
RSelenium,
polite,
glue,
here,
pdftools,
tesseract)
bis_data_path <- here(path.expand("~"), "Investigación","MacroTextMining","data","central_bank_database", "BIS")
bis_website_path <- "https://www.bis.org/"
session <- bow(bis_website_path)
session
cat(session$robotstxt$text)
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?m=256&fromDate=01%2F01%2F2017&countries=191&tillDate=31%2F12%2F2016&cbspeeches_page={page}&cbspeeches_page_length=25"))
scraping_path
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
chromever = NULL,
port = 4444L)
remote_driver <- driver[["client"]]
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
##BIS Scraper####
if(! "pacman" %in% installed.packages()){
install.packages("pacman", dependencies = TRUE)
}
pacman::p_load(tidyverse,
RSelenium,
polite,
glue,
here,
pdftools,
tesseract)
bis_data_path <- here(path.expand("~"), "Investigación","MacroTextMining","data","central_bank_database", "BIS")
bis_website_path <- "https://www.bis.org/"
session <- bow(bis_website_path)
session
cat(session$robotstxt$text)
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?m=256&fromDate=01%2F01%2F2016&countries=191&tillDate=31%2F12%2F2016&cbspeeches_page={page}&cbspeeches_page_length=25"))
scraping_path
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
chromever = NULL,
port = 4444L)
remote_driver <- driver[["client"]]
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages
metadata <- vector(mode = "list", length = nb_pages)
for(page in 1:nb_pages){
remote_driver$navigate(eval(scraping_path))
nod <- nod(session, eval(scraping_path)) # introducing to the new page
Sys.sleep(session$delay) # using the delay time set by polite
metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>%
map_chr(., ~.$getElementText()[[1]]),
info = remote_driver$findElements("css selector", ".item_date+ td") %>%
map_chr(., ~.$getElementText()[[1]]),
url = remote_driver$findElements("css selector", ".dark") %>%
map_chr(., ~.$getElementAttribute("href")[[1]]))
}
metadata <- bind_rows(metadata) %>%
separate(info, c("title", "description", "speaker"), "\n")
head(metadata)
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("[\\sitems]") %>%
as.integer()
if(nb_items == nrow(metadata)){
cat("You have extracted the metadata of all speeches")
} else{
cat("There is a problem. You have probably missed some speeches.")
}
#Download the pdf
pdf_to_download <- metadata %>%
mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
pdf_name = str_c(pdf_name, ".pdf")) %>%
pull(pdf_name)
polite_download <- function(domain, url, ...){
bow(domain) %>%
nod(glue(url)) %>%
rip(...)
}
tryCatch(
{
walk(pdf_to_download, ~polite_download(domain = bis_website_path,
url = glue("review/{.}"),
path = here(bis_data_path, "pdf"),))
},
error = function(e) {
print(glue("No pdf exist"))
}
)
View(metadata)
pdf_to_download
pdf_to_download <- pdf_to_download[!is.na(pdf_to_download)]
pdf_to_download <- pdf_to_download[!is.na(pdf_to_download)]
pdf_to_download
pdf_to_download <- metadata %>%
mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
pdf_name = str_c(pdf_name, ".pdf")) %>%
pull(pdf_name)
x<-metadata$url
#testing errors
metadata$pdffile<-substr(metadata$url,nchar(metadata$url)-8,nchar(metadata$url))
View(metadata)
#testing errors
metadata$pdffile<-substr(metadata$url,nchar(metadata$url)-12,nchar(metadata$url))
#testing errors
metadata$pdffile<-substr(metadata$url,nchar(metadata$url)-12,nchar(metadata$url))
#testing errors
metadata$pdffile<-substr(metadata$url,nchar(metadata$url)-15,nchar(metadata$url))
#testing errors
metadata$pdffile<-substr(metadata$url,27,35)
View(metadata)
#testing errors
metadata$pdffile<-substr(metadata$url,28,35)
#testing errors
metadata$pdffile<-substr(metadata$url,28,35)
metadata$pdffile<-substr(metadata$url,28,35)
metadata$pdffile<-substr(metadata$url,28,35)
metadata$pdffile<-paste(metadata$pdffile,".pdf",sep = "")
##BIS Scraper####
if(! "pacman" %in% installed.packages()){
install.packages("pacman", dependencies = TRUE)
}
pacman::p_load(tidyverse,
RSelenium,
polite,
glue,
here,
pdftools,
tesseract)
bis_data_path <- here(path.expand("~"), "Investigación","MacroTextMining","data","central_bank_database", "BIS")
bis_website_path <- "https://www.bis.org/"
session <- bow(bis_website_path)
session
cat(session$robotstxt$text)
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?m=256&fromDate=01%2F01%2F2016&countries=191&tillDate=31%2F12%2F2016&cbspeeches_page={page}&cbspeeches_page_length=25"))
scraping_path
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
chromever = NULL,
port = 4444L)
remote_driver <- driver[["client"]]
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("Page 1 of ") %>%
as.integer()
nb_pages
metadata <- vector(mode = "list", length = nb_pages)
for(page in 1:nb_pages){
remote_driver$navigate(eval(scraping_path))
nod <- nod(session, eval(scraping_path)) # introducing to the new page
Sys.sleep(session$delay) # using the delay time set by polite
metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>%
map_chr(., ~.$getElementText()[[1]]),
info = remote_driver$findElements("css selector", ".item_date+ td") %>%
map_chr(., ~.$getElementText()[[1]]),
url = remote_driver$findElements("css selector", ".dark") %>%
map_chr(., ~.$getElementAttribute("href")[[1]]))
}
metadata <- bind_rows(metadata) %>%
separate(info, c("title", "description", "speaker"), "\n")
head(metadata)
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
pluck(1) %>%
str_remove_all("[\\sitems]") %>%
as.integer()
if(nb_items == nrow(metadata)){
cat("You have extracted the metadata of all speeches")
} else{
cat("There is a problem. You have probably missed some speeches.")
}
#Download the pdf
pdf_to_download <- metadata %>%
mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
pdf_name = str_c(pdf_name, ".pdf")) %>%
pull(pdf_name)
polite_download <- function(domain, url, ...){
bow(domain) %>%
nod(glue(url)) %>%
rip(...)
}
metadata$pdffile<-substr(metadata$url,28,35)
metadata$pdffile<-paste(metadata$pdffile,".pdf",sep = "")
View(metadata)
x<-metadata$pdffile
setdiff(x,pdf_to_download)
pdf_to_download<-metadata$pdffile
tryCatch(
{
walk(pdf_to_download, ~polite_download(domain = bis_website_path,
url = glue("review/{.}"),
path = here(bis_data_path, "pdf"),))
},
error = function(e) {
print(glue("No pdf exist"))
}
)
#Download the pdf
pdf_to_download <- metadata %>%
mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
pdf_name = str_c(pdf_name, ".pdf")) %>%
pull(pdf_name)
polite_download <- function(domain, url, ...){
bow(domain) %>%
nod(glue(url)) %>%
rip(...)
}
pdf_to_download <- pdf_to_download[!is.na(pdf_to_download)]
tryCatch(
{
walk(pdf_to_download, ~polite_download(domain = bis_website_path,
url = glue("review/{.}"),
path = here(bis_data_path, "pdf"),))
},
error = function(e) {
print(glue("No pdf exist"))
}
)
shiny::runApp('Investigación/Plural Estudio/dashboards/diagnosticoGE/diganosticoGE')
shiny::runApp('Investigación/Plural Estudio/dashboards/diagnosticoGE/diganosticoGE')
shiny::runApp('Investigación/Plural Estudio/dashboards/diagnosticoGE/diganosticoGE')
